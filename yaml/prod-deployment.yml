$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: langgraph-agent-deployment-prod
endpoint_name: langgraph-agent-endpoint
model: azureml:langgraph-agent-pf-model@latest
  # You can also specify model files path inline
  # path: examples/flows/chat/basic-chat
environment: azureml:langgraph-agent-env@latest
  # image: mcr.microsoft.com/azureml/promptflow/promptflow-runtime:20240403.v2
  # image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04
  # inference config is used to build a serving container for online deployments
  # inference_config:
  #  liveness_route:
  #    path: /health
  #    port: 8080
  #  readiness_route:
  #    path: /health
  #    port: 8080
  #  scoring_route:
  #    path: /score
  #    port: 8080
request_settings:
    request_timeout_ms: 180000
    max_concurrent_requests_per_instance: 10
readiness_probe:
    initial_delay: 20
    period: 10
    timeout: 5
instance_type: Standard_DS2_v2
instance_count: 1
app_insights_enabled: true
environment_variables:

  # "compute" mode is the default mode, if you want to deploy to serving mode, you need to set this env variable to "serving"
  PROMPTFLOW_RUN_MODE: serving

  # for pulling connections from workspace
  PRT_CONFIG_OVERRIDE: deployment.subscription_id=b7f4c69c-9a42-41f7-962f-bd704c8114d6,deployment.resource_group=CAGRIKURT,deployment.workspace_name=aml-ckurt,deployment.endpoint_name=langgraph-agent-endpoint,deployment.deployment_name=langgraph-agent-deployment-prod

  # (Optional) When there are multiple fields in the response, using this env variable will filter the fields to expose in the response.
  # For example, if there are 2 flow outputs: "answer", "context", and I only want to have "answer" in the endpoint response, I can set this env variable to '["answer"]'.
  # If you don't set this environment, by default all flow outputs will be included in the endpoint response.
  # PROMPTFLOW_RESPONSE_INCLUDED_FIELDS: '["category", "evidence"]'